---
title: "Formats and filters"
author: "By Matt Waite"
output: html_notebook
---

What format R reads your data will dictate what you can do with it. R, like many data analysis tools, has several formats -- character, integer, numeric, factors and dates are the most common. Character fields are text -- like names. Integer are whole numbers -- no decimals. Numeric is more forgiving -- you can have decimals. Dates are what you think they are, but they come with special characteristics. R understands date fields and you can do things like measure the number of days between two dates, or libraries will interpret years, months and days as they are. If dates are imported as character fields, that benefit won't come with it.

Factors are an array of data in a single field. So you could store a sequence of numbers, or a list of names, in a factor in a single field. In my experience, you'll most often experience factors when the numbers in you number column have comma separators for the thousands. R will read 1,000 and 2,000 as a factor with two elements -- a 1 and a 0, for instance.

Let's look at this in action.

You've been assigned to look at the counties in the United States that produce soy beans as part of a story about President Trump's trade war with China. You download a [data file](https://www.dropbox.com/s/mgnyi11amq9psub/soybeans2012.csv?dl=0) from the United States Department of Agriculture's Census of Agriculture that breaks down the total production of soybeans by county. This should be simple, right? Just load the file and sort it to find the counties that produce the most soybeans.

Let's see how easy it is.
```{r}
soybeans <- read.csv("../Data/soybeans2012.csv")
head(soybeans)
```
Note the `Value` field. Note it's not a number. It's a factor. Why? Because the thousand separator is a comma, and R reads that comma literally. Also, if you scroll down a bit, you'll find that the USDA didn't report data for some counties because there were so few farmers that there's a possibility you could identify them, a process call suppression. In those counties, you'll see "(D)". Which does us no good. We want them to be blank.

So how do we fix this?

There's two ways.

1. If you were going to do this analysis regularly, you'd want to script this. You'd use R to create a new column, remove the comma and coerce it to a numeric value. That sounds like a lot, but it's really not. We're going to use `dplyr` and a tool in R called `gsub` which replaces a pattern with whatever you want. In our case, we want to replace a comma and the (D) with nothing. We can also force a field into a particular format by casting it with as.X, where X is numeric, integer, date or character. To force that change, we need to do that first -- we wrap the result of our gsub in the type change to make the result that thing.

So I'm going to make a new variable called cleanbeans and populate it with my dataframe with a new field called NumVal, which is my Value field with commas and (D) removed.

```{r}
library(dplyr)
cleanbeans <- soybeans %>% mutate(
  NumVal = as.numeric(gsub(",|(D)", "", Value))
)
head(cleanbeans)
```

2. If this is a one time analysis, I have a favorite saying: "Done is better than clever."

Open the file in Excel, click on the value column (column T) and go to Edit > Find > Replace ... and replace (D) with nothing. Then go to Format > Cells and click Number as the category and uncheck use 1000 separator if it is (it isn't by default). Then save the file as a Comma Separated Values file and open it in R.

For replicability, Option 2 is Bad. It can't be repeated as easily as the first option. Or if the USDA releases new data while you are on deadline. But sometimes, you just need to get it done now -- when deadline is upon you -- and that's one way. Just don't make it a habit.

### Working with dates

One of the most frustrating things in data is working with dates. Everyone has a different opinion on how to record them, and every software package on the planet has to sort it out. Dealing with it can be a little ... confusing. And every dataset has something new to throw at you. So consider this an introduction.

We're going to do this two ways. First we're going to use base R to solve a tricky problem. And then we'll use a library called `lubridate` to solve a more common and less tricky problem.

First, we'll import `dyplyr` like we always do.
```{r}
library(dplyr)
```
Next we'll use the UNL crime data in the Data folder. This time, we want to see how many incidents are reported every year to UNLPD and how much that has changed.

With this import, we're going to add a directive to the importer -- `stringsAsFactors = False`, which is going to tell the importer that anything you think is a factor -- the array of objects discussed above -- is actually just a string of characters, so don't import it as a factor.
```{r}
unlcrime <- read.csv("../Data/unlcrime.csv", stringsAsFactors = FALSE)
head(unlcrime)
```
The code to fix dates is a little convoluted, but once you learn the pattern, it's not so bad. We're going to do this in steps.

First, we're going to use `mutate` in dplyr to create a new field with properly formatted dates. `mutate` just means let's create a new field. We can do a lot with mutate, but in this case, we just want to copy data from one field, reformat it, and then put it in another field.

Here's what that looks like:
```{r}
unlcrime %>% mutate(
    CleanDate = as.POSIXct(Reported, format="%Y-%m-%dT%H:%M")
)
```
Okay, let's walk through this. By now, you should understand mutating. So the bits that are new are these:

`CleanDate = as.POSIXct(Reported, format="%Y-%m-%dT%H:%M")`

So `CleanDate` is the name of the new field we're creating.

The `as.POSIXct` takes some explaining. And there's [a lot more explaining here](http://neondataskills.org/R/time-series-convert-date-time-class-POSIX/). But base R has a couple of ways of parsing dates. The most common one you'll see is `as.Date` which works great, unless you have dates AND times like we do. So you have to use one of two `as.POSIX` variants, `as.POSIXct` and `as.POSIXlt`, and knowing the difference between them isn't useful. So `as.POSIXct` takes some inputs -- the field you're working with, and format of the date field, using some pattern markers that makes sense when you learn what they are. So our date field is `Reported`, so that's what we're using to supply the input data. The format of our data, if we look at the output of head above, is YYYY-MM-DDTHH:MM:SS, or year-month-dayThours:minutes:seconds. The oddball here is the T in the middle. If that wasn't there, we'd have an easier time converting this. But it's there, for a reason some developer somewhere decided, and we have to deal with it.

So to parse that pattern, we use a set of markers that tell R what this thing in this spot is. You can get all of the markers that are possible in the [strptime documentation](https://stat.ethz.ch/R-manual/R-devel/library/base/html/strptime.html), but what we have here represents the most common. So `%Y` means four digit year (versus `%y` which is just a two digit year). The rest are pretty self explanator after that, but notice that the dashes, the T and the colons in the time are just typed into it. They too are part of the pattern.  

But our question is to count up the total number of incidents per year. So we need to get just a year back, group them together by year and count them. We can do this using `format` and the same markers as before. It looks like this:
```{r}
unlcrime %>% mutate(
    year = format(as.POSIXct(Reported, format="%Y-%m-%dT%H:%M"), "%Y")
)
```
So, now we can add the group by and summarize steps to that, and we get our answer.
```{r}
unlcrime %>% mutate(
  year = format(as.POSIXct(Reported, format="%Y-%m-%dT%H:%M"), "%Y")) %>%
  group_by(year) %>%
  summarize(
    count = n()
)
```
### When it's not weird, Lubridate
There is a [library called lubridate](https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html) that does some of this parsing for you. Let's get some of the bits we're familiar with out of the way. First we'll open the library, then import a dataset of parking tickets.
```{r}
library(lubridate)
tickets <- read.csv("../Data/tickets.csv", stringsAsFactors = FALSE)
head(tickets)
```
This is easy enough that it barely requires explanation. If your date format matches one of lubridate's patterns, you can just apply that pattern. It's just this simple.
```{r}
tickets %>% mutate(
    CleanDate = ymd_hms(Date)
)
```
Now it might not look like anything is different, but it is. CleanDate can now be used as dates, you can do date math with it, you can parse it to get the year or month or whatever you need. It becomes useful data.

### Filtering data

Now, returning to our soybean data from the earlier example, what if we just wanted to see Nebraska? Or whatever state you are interested in?

To do that, we use dplyr's `filter`, which does what it says on the tin. You can simply filter the things you want (or don't want) so your numbers reflect the things you are just looking at. So in this case, we're going to get all the records where the State equals "NEBRASKA".

```{r}
library(dplyr)

soybeans <- read.csv("../Data/soybeans2012.csv")

cleanbeans <- soybeans %>%
mutate(
  NumVal = as.numeric(gsub(",|(D)", "", Value))
) %>%
filter(State == "NEBRASKA")

head(cleanbeans)
```


### Working with Dates Assignment

Load in [this dataset](https://www.dropbox.com/s/yd54u1dr2a6kt3m/LPDassaults.csv?dl=0) of all assaults on Lincoln Police officers and answer this question: What month do most assaults on police officers happen?

#### Rubric

1. Did you load the data correctly?
2. Did you apply the right date parsing formula?
3. Did you parse the month out of the formatted data?
4. Did you do the group and count correctly?
5. Did you describe your steps in Markdown comments?

### Filtering Assignment

Take the salary data you used in the aggregates assigment. Now I want you to calculate the mean and median salaries of the most common job titles at the University of Nebraska-Lincoln.

#### Rubric

1. Did you read the data into a dataframe?
2. Did you use group by syntax correctly?
3. Did you use summarize syntax correctly?
4. Did you use the sorting syntax correctly?
5. Did you use filter syntax correctly?
6. Did you use Markdown comments to explain your steps?
